---
title: "Biostats 626 midterm"
author: "Yundan Zhang"
date: "2023-03-24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(glmnet)
library(rpart)
library(caret)
library(e1071)
```
## Data Cleaning
```{r}
## Import dataset
# The origin dataset report error that colnames have different length with columns
# Create a list to store the colnames
traincol <- c("subject", "activity", paste0("F", 1:561))
dtrain <- read.delim("data/training_data.txt", sep = "\t", header = F, col.names = traincol)
# Remove original colnames
dtrain <- dtrain[-1, ]
testcol <- c("subject", paste0("F", 1:561))
dtest <- read.delim("data/test_data.txt", sep = "\t", fill = F, col.names = testcol)
dtest <- dtest[-1, ]

## Create binary variables
# Set 0 as static and 1 as active
dtrain <- dtrain %>%
  mutate(activity = ifelse(activity %in% 7:12, 7, activity)) %>%
  mutate(status = case_when(
    activity %in% c(1:3) ~ 1,
    activity %in% 4:7 ~ 0,
    TRUE ~ NA_real_
  ))
```

```{r}
## Data standardization
standardized_X = dtrain[, 3:563]
# standardized_X = scale(dtrain[, 3:563])
val = 1:1000
train_X = standardized_X[-val ,]
val_X = standardized_X[val ,]
Train_YB = dtrain[, 564]
Train_YM = dtrain[, 2]
train_YB = Train_YB [-val]
val_YB = Train_YB [val]
train_YM = Train_YM [-val]
val_YM = Train_YM [val]
test_X = dtest[, 2:562]
# test_X = scale(dtest[, 2: 562])
```


## Feature Selection

```{r}
## Lasso Regression for binary classification
cvfit_lB <- cv.glmnet(as.matrix(train_X), train_YB, family = "binomial", alpha = 1)
lambda_lB <- cvfit_lB$lambda.min
c1 <- coef(cvfit_lB, s = lambda_lB)
flb <- row.names(c1)[which(c1 != 0)]
flb <- flb[-1]
# flb

train_X <- train_X[, flb]
val_X <- val_X[, flb]
```

```{r}
## PCA for classification
pca_B <- prcomp(train_X, scale. = F)
plot(pca_B, type = 'l')
cumulative_variance <- cumsum(pca_B$sdev^2 / sum(pca_B$sdev^2))
num_components <- which.max(cumulative_variance > 0.90)

train_X <- predict(pca_B, train_X)[, 1:num_components]
val_X <- predict(pca_B, val_X)[, 1:num_components]

```


## Binary Classification
```{r}
## Decision Tree
# Train the decision tree model
tb_model <- rpart(train_YB ~ ., data = data.frame(train_YB, train_X), method = "class")

# Make predictions on the validation set
val_PB <- predict(tb_model, newdata = data.frame(val_X), type = "class")

# Compute confusion matrix
cm1 <- confusionMatrix(table(val_PB, val_YB))

# Print evaluation metrics
print(cm1)
```

```{r}
## SVM with kernal function
train_YB <- factor(train_YB, levels = c(0, 1))
svm_B <- svm(train_X, train_YB, kernel = "linear")
val_PB <- predict(svm_B, newdata = val_X)
cm2 <- confusionMatrix(table(val_PB, val_YB))
print(cm2)
```

## Multi-Classification
```{r}
## SVM with kernal function
train_YM <- factor(train_YM, levels = 1:7)
svm_M <- svm(train_X, train_YM, type = "C-classification", kernel = "polynomial")
val_PM <- predict(svm_M, newdata = val_X)
cm3 <- confusionMatrix(table(val_PM, val_YM))
print(cm3)
```

```{r}
## Hyperparameter tune
# Define the search grid
tune_grid <- expand.grid(
  C = 10^(-1:1),
  kernel = c("linear", "polynomial", "radial", "sigmoid"))

# Perform cross-validated hyperparameter tuning
tune_result <- tune(svm, train_X, train_YM, ranges = tune_grid, tunecontrol = tune.control(sampling = "cross", cross = 5))

# Train the SVM model with the best cost parameter
best_svm <- tune_result$best.model

print(tune_result$best.parameters)

# Make predictions on the validation set
val_PM_optimized <- predict(best_svm, newdata = val_X)

# Compute the confusion matrix
cm4 <- confusionMatrix(table(val_PM_optimized, val_YM))
print(cm4)

```

## Model Finalization
```{r}
## Final PCA model
pca_NB <- prcomp(standardized_X, scale. = F)
cumulative_variance <- cumsum(pca_NB$sdev^2 / sum(pca_NB$sdev^2))
num_components2 <- which.max(cumulative_variance > 0.90)

standardized_X <- predict(pca_NB, standardized_X)[, 1:num_components2]
test_X <- predict(pca_NB, test_X)[, 1:num_components2]
```

```{r}
## Binary Classification
Train_YB <- factor(Train_YB, levels = c(0, 1))
Best_B <- svm(standardized_X, Train_YB, kernel = "linear")
pred_B <- predict(Best_B, newdata = test_X)
pred_B <- data.frame(predictions = pred_B)
write.table(pred_B, file = "binary_yd.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
```

```{r}
## Multi Classification
Train_YM <- factor(Train_YM, levels = 1:7)
Best_M <- svm(standardized_X, Train_YM, type = "C-classification", kernel = "radial", C = 0.1)
pred_M <- predict(Best_M, newdata = test_X)
pred_M <- data.frame(predictions = pred_M)
write.table(pred_M, file = "multiclass_yd.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
```